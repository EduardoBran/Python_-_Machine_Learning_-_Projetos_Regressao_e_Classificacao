








# Manipulação de Dados
import numpy as np
import pandas as pd

# Criação e Visualização de Gráficos
import seaborn as sns
import matplotlib.pyplot as plt

# Carrega a função SMOTE (Balanceamento de Classes)
from imblearn.over_sampling import SMOTE

# Label Enconding
from sklearn.preprocessing import LabelEncoder

# Pacote do Python para Machine Learning
import sklearn

# Função model_selection do pacote sklearn
from sklearn.model_selection import train_test_split    # dividir os dados em treino e teste
from sklearn.model_selection import GridSearchCV        # técnica de utilização de hiperparâmetros
from sklearn.model_selection import RandomizedSearchCV  # técnica de utilização de hiperparâmetros
from sklearn.model_selection import cross_val_score     # validação cruzada (para avaliação do modelo)

# Padronização
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Pacotes do sklearn com os Algoritmos

# Função com Algoritmo de ML (Regressão Logística)
from sklearn.linear_model import LogisticRegression     

# Função com Algoritmo de ML (RandomForest)
from sklearn.ensemble import RandomForestClassifier

# Função com Algoritmo de ML (KNN)
from sklearn.neighbors import KNeighborsClassifier

# Função com Algoritmo de ML (Árvores de Decisão)
from sklearn.tree import DecisionTreeClassifier

# Função com Algoritmo de ML (SVC)
from sklearn.svm import SVC

# Função com Algoritmo de ML (GBM)
from sklearn.ensemble import GradientBoostingClassifier

# Função com Algoritmo de ML (AdaBoost)
from sklearn.ensemble import AdaBoostClassifier

# Função com Algoritmo de ML (LGBMClassifier)
from lightgbm import LGBMClassifier 

# Função com Algoritmo de ML (XGBClassifier)
from xgboost import XGBClassifier

# Função com Algoritmo de ML (Naive Bayes)
from sklearn.naive_bayes import GaussianNB

# Função com Algoritmo de ML (Neural Networks)
from sklearn.neural_network import MLPClassifier

# Função com Algoritmo de ML (CatBoost)
from catboost import CatBoostClassifier

# Função com Algoritmo de ML (Extra Trees (Extremely Randomized Trees)
from sklearn.ensemble import ExtraTreesClassifier

# Função com Algoritmo de ML (Voting Classifier)
from sklearn.ensemble import VotingClassifier


# Calcular as Métricas para Avaliação dos Modelos
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix
from sklearn.metrics import accuracy_score

# Salvar o modelo após o treinamento
import joblib
import pickle

# Indicando que os Gráficos apareçam no Jupyter Notebook
%matplotlib inline

# Mensagens de Aviso
import warnings
warnings.filterwarnings("ignore")





# Carregar o arquivo de dados com caminho relativo
file_path = 'dados/adult.data'

# Definir os nomes das colunas, já que o arquivo 'adult.data' não tem cabeçalho
column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 
                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 
                'hours_per_week', 'native_country', 'income']

# Carregar os dados
dados = pd.read_csv(file_path, header=None, names=column_names, na_values=' ?', sep=',\s*', engine='python')

# Exibir as 10 linhas aleatóras
dados.sample(5)





#import webbrowser
import os
## Gerar um arquivo HTML com a tabela no diretório atual
#dados.to_html('tabela_dados.html')

# Abrir o arquivo HTML no navegador em uma nova aba
#file_path = os.path.abspath('tabela_dados.html')
#webbrowser.open_new_tab(f'file://{file_path}')





# Informações do DataFrame
print('\n-------------------------------------------------------------------------------------------\n\n')
print('INFO\n\n')
dados.info()

# Verifica se há valores ausentes e duplicados
valores_ausentes = dados.isna().sum().sum() > 0
valores_duplicados = dados.duplicated().sum() > 0

# Nomes das variáveis com valores ausentes
variaveis_ausentes = dados.columns[dados.isna().any()].tolist()

# Número de linhas duplicadas
num_linhas_duplicadas = dados.duplicated().sum()

# Porcentagem de linhas duplicadas
porcentagem_linhas_duplicadas = (num_linhas_duplicadas / len(dados)) * 100

# Exibe o resultado
if valores_ausentes:
    print("\n\nExistem valores ausentes:", valores_ausentes)
    print("Variáveis com valores ausentes:", variaveis_ausentes)
else:
    print("\n\nNenhuma variável possui valores ausentes.")

if valores_duplicados:
    print("\nExistem valores duplicados:", valores_duplicados)
    print("Número de Linhas Duplicadas:", num_linhas_duplicadas)
    print("Porcentagem de Linhas Duplicadas: {:.2f}%\n\n".format(porcentagem_linhas_duplicadas))
else:
    print("\nNenhuma variável possui valores duplicados.\n\n")


# Verificar quais colunas têm valores "?"
columns_with_question_marks = dados.columns[(dados == '?').any()].tolist()

# Contar o número total de ocorrências de "?"
total_question_marks = dados.isin(['?']).sum().sum()

# Contar o número de linhas que contêm pelo menos um valor "?"
rows_with_question_marks = (dados == '?').any(axis=1).sum()

# Exibir o resultado
print("\nColunas com valores '?':", columns_with_question_marks)
print("Número total de ocorrências de '?':", total_question_marks)
print("Número total de linhas com valores '?':", rows_with_question_marks)





# Renomear a coluna 'fnlwgt' para 'sample_weight'
dados.rename(columns={'fnlwgt': 'sample_weight'}, inplace=True)

dados.columns





# Exibindo Variáveis Categóricas (filtrando)
dados.dtypes[dados.dtypes == 'object']


# Exibindo Variáveis Numéricas (filtrando)
dados.dtypes[dados.dtypes != 'object']





# Salvar o formato original
original_float_format = pd.options.display.float_format

# Ajustar a exibição do pandas para valores sem notação científica
pd.options.display.float_format = '{:.2f}'.format

# Verificando o resumo estatístico sem notação científica
print('\nSem Notação Científica')
display(dados.describe())
print('\n----------------------------------------------------------------------------------------------\n\n')

# Restaurar o formato original
pd.options.display.float_format = original_float_format

# Verificando o resumo estatístico novamente para confirmar que voltou ao normal
print('\nCom Notação Científica')
display(dados.describe())





# Plot
dados.hist(figsize = (15,15), bins = 10) 
plt.show()








# Describe (informando que é para somente variáveis categóricas)
print('\nDescribe\n')
display(dados.describe(include = ['object']))
print('\n------------------------------------------------------------------------\n\n')

# Verificando Tipo das Variáveis
print('\nTipo das Variáveis\n')
print(dados['workclass'].unique())
print(dados['education'].unique())
print(dados['marital_status'].unique())
print(dados['occupation'].unique())
print(dados['relationship'].unique())
print(dados['race'].unique())
print(dados['sex'].unique())
print(dados['native_country'].unique())
print(dados['income'].unique())
print('\n------------------------------------------------------------------------\n\n')





import matplotlib.pyplot as plt
import seaborn as sns

# Exibir os gráficos de contagem para todas as variáveis categóricas
categorical_vars = ['workclass', 'education', 'marital_status', 'occupation', 
                    'relationship', 'race', 'sex', 'native_country', 'income']

for var in categorical_vars:
    plt.figure(figsize=(10, 6))
    sns.countplot(data=dados, x=var, label='Count')
    plt.title(f'Contagem de {var}')
    plt.xlabel(var.replace('_', ' ').capitalize())
    plt.ylabel('Contagem')
    plt.xticks(rotation=45)
    plt.show()

# Value counts para todas as variáveis categóricas
for var in categorical_vars:
    var_counts = dados[var].value_counts()
    print(f"\nContagem de {var.replace('_', ' ').capitalize()}:")
    for category, count in var_counts.items():
        print(f'{category}: {count}')





# Describe (informando que é para somente a variável income)
print('\nDescribe\n')
print(dados['income'].describe())
print('\n------------------------------------------------------------------------\n')

# Verificando Tipo das Variáveis
print('\nTipo das Variável\n')
print(dados['income'].unique())
print('\n------------------------------------------------------------------------\n\n')

# Exibindo o gráfico de contagem para 'income'
plt.figure(figsize=(10, 6))
sns.countplot(data=dados, x='income', label='Count')
plt.title('Contagem de Income')
plt.xlabel('Income')
plt.ylabel('Contagem')
plt.show()

# Value counts para a variável 'income'
income_counts = dados['income'].value_counts()
print("Contagem de Income:")
for income, count in income_counts.items():
    print(f'{income}: {count}')





# Substituindo valores faltantes por 'Desconhecido'
dados['workclass'].replace('?', 'Desconhecido', inplace=True)
dados['occupation'].replace('?', 'Desconhecido', inplace=True)
dados['native_country'].replace('?', 'Desconhecido', inplace=True)

# Verificar quais colunas têm valores "?"
columns_with_question_marks = dados.columns[(dados == '?').any()].tolist()

# Contar o número total de ocorrências de "?"
total_question_marks = dados.isin(['?']).sum().sum()

# Contar o número de linhas que contêm pelo menos um valor "?"
rows_with_question_marks = (dados == '?').any(axis=1).sum()

# Exibir o resultado
print("\nColunas com valores '?':", columns_with_question_marks)
print("Número total de ocorrências de '?':", total_question_marks)
print("Número total de linhas com valores '?':", rows_with_question_marks)


dados.sample(10)





# Lista de variáveis categóricas
categorical_vars = ['workclass', 'education', 'marital_status', 'occupation', 
                    'relationship', 'race', 'sex', 'native_country', 'income']

# Aplicando Label Encoding
label_encoders = {}
for var in categorical_vars:
    le = LabelEncoder()
    dados[var] = le.fit_transform(dados[var])
    label_encoders[var] = le


# Análise estatística após Label Encoding
print('\nResumo Estatístico Após Label Encoding\n')
display(dados.describe())

# Verificando os tipos das variáveis
print('\nTipos das Variáveis Após Label Encoding\n')
print(dados.dtypes)

# Verificando os valores únicos de 'income'
print('\nValores Únicos da Variável Alvo (income)\n')
print(dados['income'].unique())





# Verifica se há valores ausentes (por coluna)
dados.isna().sum()





# Shape
print(dados.shape)

# Verifica quantidade de linhas com valores duplicados
print(dados.duplicated().sum())


# Remove registros duplicados (remove uma das duplicatas)
dados = dados.drop_duplicates()

# Shape
print(dados.shape)





# Nome das variáveis
dados.columns


dados.describe()





# Describe
print(dados['sample_weight'].describe())

print(dados.shape)
print('\n--------------------------------------------------------------------------------------------------\n\n')

# Boxplot
sns.boxplot(dados.sample_weight)





# Calculando IQR
Q1 = dados['sample_weight'].quantile(0.25)
Q3 = dados['sample_weight'].quantile(0.75)
IQR = Q3 - Q1

# Definindo os limites
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Aplicando o tratamento
dados['sample_weight'] = np.where(dados['sample_weight'] > upper_bound, upper_bound, 
                                  np.where(dados['sample_weight'] < lower_bound, lower_bound, dados['sample_weight']))

# Verificando o resultado
print(dados['sample_weight'].describe())

# Exibindo o boxplot novamente
sns.boxplot(dados['sample_weight'])
plt.show()





# Describe
print(dados['capital_gain'].describe())

print(dados.shape)
print('\n--------------------------------------------------------------------------------------------------\n\n')

# Boxplot
sns.boxplot(dados.capital_gain)





from sklearn.ensemble import IsolationForest

# Instanciando o modelo Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)

# Ajustando o modelo aos dados
iso_forest.fit(dados[['capital_gain']])

# Identificando as anomalias (outliers)
dados['anomaly'] = iso_forest.predict(dados[['capital_gain']])

# Substituindo os outliers (anomaly == -1) pelos limites inferiores/superiores
upper_bound = dados[dados['anomaly'] == 1]['capital_gain'].max()
lower_bound = dados[dados['anomaly'] == 1]['capital_gain'].min()

dados['capital_gain'] = np.where(dados['anomaly'] == -1, upper_bound, dados['capital_gain'])

# Removendo a coluna de anomalia
dados.drop(columns=['anomaly'], inplace=True)

# Verificando o resultado
print(dados['capital_gain'].describe())

# Exibindo o boxplot novamente
sns.boxplot(dados['capital_gain'])
plt.show()








# Describe
print(dados['capital_loss'].describe())

print(dados.shape)
print('\n--------------------------------------------------------------------------------------------------\n\n')

# Boxplot
sns.boxplot(dados.capital_loss)





from sklearn.ensemble import IsolationForest

# Instanciando o modelo Isolation Forest
iso_forest = IsolationForest(contamination=0.03, random_state=42)

# Ajustando o modelo aos dados
iso_forest.fit(dados[['capital_loss']])

# Identificando as anomalias (outliers)
dados['anomaly'] = iso_forest.predict(dados[['capital_loss']])

# Substituindo os outliers (anomaly == -1) pelos limites superiores
upper_bound = dados[dados['anomaly'] == 1]['capital_loss'].max()
dados['capital_loss'] = np.where(dados['anomaly'] == -1, upper_bound, dados['capital_loss'])

# Removendo a coluna de anomalia
dados.drop(columns=['anomaly'], inplace=True)

# Verificando o resultado
print(dados['capital_loss'].describe())

# Exibindo o boxplot novamente
sns.boxplot(dados['capital_loss'])
plt.show()





# Describe
print(dados['hours_per_week'].describe())

print(dados.shape)
print('\n--------------------------------------------------------------------------------------------------\n\n')

# Boxplot
sns.boxplot(dados.hours_per_week)





# Calculando IQR
Q1 = dados['hours_per_week'].quantile(0.25)
Q3 = dados['hours_per_week'].quantile(0.75)
IQR = Q3 - Q1

# Definindo os limites
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Aplicando o tratamento
dados['hours_per_week'] = np.where(dados['hours_per_week'] > upper_bound, upper_bound, 
                                  np.where(dados['hours_per_week'] < lower_bound, lower_bound, dados['hours_per_week']))

# Verificando o resultado
print(dados['hours_per_week'].describe())

# Exibindo o boxplot novamente
sns.boxplot(dados['hours_per_week'])
plt.show()



dados.describe()





# Info
dados.info()


# Verificando Correlação (tabela)
dados.corr()


# Visualizando Correlações através de um Mapa de Calor

# Criando o heatmap
corr_matrix = dados.select_dtypes(include=[np.number]).corr()

plt.figure(figsize=(10, 8))  # Define o tamanho da figura
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', vmin=-1, vmax=1, cbar=True, square=True)
plt.title('Mapa de Calor das Correlações')
plt.show()








# Cria um objeto separado para a variável alvo
y = dados.income

# Cria um objeto separado para as variáveis de entrada
X = dados.drop('income', axis=1)

# Split em dados de treino e teste sem amostragem estratificada
X_treino, X_teste, y_treino, y_teste = train_test_split(X, 
                                                        y, 
                                                        test_size=0.2, 
                                                        random_state=1234)

# Print do shape
print(X_treino.shape, X_teste.shape, y_treino.shape, y_teste.shape)





# Value Counts
dados['income'].value_counts()





# Importar bibliotecas necessárias
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

# Dividir os dados em conjunto de treino e teste
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=1234)

# Definir os valores de k_neighbors a serem testados
k_neighbors_values = [3, 5, 7, 9]

# Criar o pipeline com SMOTE e um modelo de classificação
pipeline = Pipeline([
    ('smote', SMOTE(random_state=1234)),
    ('classifier', RandomForestClassifier(random_state=1234))
])

# Definir os parâmetros para o GridSearch
param_grid = {
    'smote__k_neighbors': k_neighbors_values
}

# Configurar o GridSearchCV
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Ajustar o GridSearch ao conjunto de treino
grid_search.fit(X_treino, y_treino)

# Imprimir o melhor valor de k_neighbors
print("Melhor valor de k_neighbors: ", grid_search.best_params_['smote__k_neighbors'])

print('\n-----------------------------------------------------------------------------------------\n')

# Avaliar o modelo no conjunto de teste
y_pred = grid_search.best_estimator_.predict(X_teste)
print(classification_report(y_teste, y_pred))


# Vamos aplicar a técnica de oversampling e aumentar o número de exemplos da classe minoritária
over_sampler = SMOTE(k_neighbors=7, random_state=42)

# Aplica o oversampling (deve ser feito somente com dados de treino)
X_res, y_res = over_sampler.fit_resample(X_treino, y_treino)

print(y_treino.value_counts())
print(X_treino.shape)
print(y_treino.shape)

print('\n==================================================\n')

# Ajusta o nome do dataset de treino para X
X_treino = X_res
# Ajusta o nome do dataset de treino para y
y_treino = y_res

print(y_treino.value_counts())
print(X_treino.shape)
print(y_treino.shape)





from sklearn.preprocessing import StandardScaler

# Definindo as variáveis contínuas que precisam ser padronizadas
variaveis_continuas = ['age', 'sample_weight', 'capital_gain', 'capital_loss', 'hours_per_week']

# Inicializando o scaler
scaler = StandardScaler()

# Aplicando a padronização nas variáveis contínuas do conjunto de treino
X_treino_padronizado = X_treino.copy()
X_treino_padronizado[variaveis_continuas] = scaler.fit_transform(X_treino_padronizado[variaveis_continuas])

# Aplicando a padronização nas variáveis contínuas do conjunto de teste
X_teste_padronizado = X_teste.copy()
X_teste_padronizado[variaveis_continuas] = scaler.transform(X_teste_padronizado[variaveis_continuas])

# Verificando o resultado
display(X_treino_padronizado.sample(5))
display(X_teste_padronizado.sample(5))





# Cria um dataframe para receber as métricas de cada modelo
df_modelos = pd.DataFrame()





# Configuração do pipeline de pré-processamento e modelo
pipeline = Pipeline([
    ('scaler', StandardScaler()),                    # Adiciona o escalonador
    ('logistic', LogisticRegression(max_iter=1000))  # Aumenta max_iter para garantir a convergência
])

# Define lista de hiperparâmetros
tuned_params_RL = {
    'logistic__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
    'logistic__penalty': ['l2']  # 'l1' está disponível apenas com o solucionador 'liblinear'
}

# Criando modelos com GridSearch
grid_search = GridSearchCV(pipeline, 
                         tuned_params_RL, 
                         scoring='roc_auc', 
                         n_jobs=-1)
grid_search.fit(X_treino_padronizado, y_treino)

# Visualizando o melhor modelo
grid_search.best_estimator_





## Criando e Treinando o Modelo

# Obtendo os melhores hiperparâmetros encontrados
best_params = grid_search.best_params_

# Criando o Modelo
modelo_RL = LogisticRegression(
    C=best_params['logistic__C'], 
    penalty='l2', 
    max_iter=1000
)

# Treinando o Modelo
print(modelo_RL.fit(X_treino, y_treino))

print('\n=================================================================================\n')


## Previsões

print('\nPrevisões\n\n')

# Previsões com dados de teste
y_pred_RL = modelo_RL.predict(X_teste)
print('Previsões de Classe:')
print(y_pred_RL[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_RL = modelo_RL.predict_proba(X_teste)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_RL[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_RL_pos = modelo_RL.predict_proba(X_teste)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_RL_pos[:5])

print('\n\n=================================================================================\n')


## Avaliação do Modelo

print('\nAvaliação do Modelo\n\n')

# Matriz de confusão
conf_matrix_RL = confusion_matrix(y_teste, y_pred_RL)
print('Matriz de Confusão:')
print(conf_matrix_RL)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_RL, fp_RL, fn_RL, tp_RL = conf_matrix_RL.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_RL = roc_auc_score(y_teste, y_pred_proba_RL_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_RL, tpr_RL, thresholds_RL = roc_curve(y_teste, y_pred_proba_RL_pos)

# AUC em teste
auc_RL = auc(fpr_RL, tpr_RL)

# Acurácia em teste
acuracia_RL = accuracy_score(y_teste, y_pred_RL)

# Exibindo as métricas
print(f'Acurácia: {acuracia_RL:.4f}')
print(f'ROC AUC Score: {roc_auc_RL:.4f}')
print(f'AUC Score: {auc_RL:.4f}')





# Salvando as métricas em um DataFrame
df_modelos = pd.DataFrame({
    'Nome do Modelo': ['Modelo 1 - Não Padronizado'],
    'Nome do Algoritmo': ['Logistic Regression'],
    'ROC_AUC Score': [roc_auc_RL],
    'AUC Score': [auc_RL],
    'Acuracia': [acuracia_RL]    
})

display(df_modelos)





# Configuração do pipeline de pré-processamento e modelo
pipeline = Pipeline([
    ('scaler', StandardScaler()),                    # Adiciona o escalonador
    ('logistic', LogisticRegression(max_iter=1000))  # Aumenta max_iter para garantir a convergência
])

# Define lista de hiperparâmetros
tuned_params_RL = {
    'logistic__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
    'logistic__penalty': ['l2']  # 'l1' está disponível apenas com o solucionador 'liblinear'
}

# Criando modelos com GridSearch
grid_search = GridSearchCV(pipeline, 
                         tuned_params_RL, 
                         scoring='roc_auc', 
                         n_jobs=-1)
grid_search.fit(X_treino_padronizado, y_treino)

# Visualizando o melhor modelo
print(grid_search.best_estimator_)





## Criando e Treinando o Modelo

# Obtendo os melhores hiperparâmetros encontrados
best_params = grid_search.best_params_

# Criando o Modelo
modelo_RL_padronizado = LogisticRegression(
    C=best_params['logistic__C'], 
    penalty='l2', 
    max_iter=1000
)

# Treinando o Modelo
print(modelo_RL_padronizado.fit(X_treino_padronizado, y_treino))

print('\n=================================================================================\n')


## Previsões

print('\nPrevisões\n\n')

# Previsões com dados de teste
X_teste_padronizado = X_teste.copy()
X_teste_padronizado[variaveis_continuas] = scaler.transform(X_teste_padronizado[variaveis_continuas])

y_pred_RL_padronizado = modelo_RL_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_RL_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_RL_padronizado = modelo_RL_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_RL_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_RL_pos_padronizado = modelo_RL_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_RL_pos_padronizado[:5])

print('\n\n=================================================================================\n')


## Avaliação do Modelo

print('\nAvaliação do Modelo\n\n')

# Matriz de confusão
conf_matrix_RL_padronizado = confusion_matrix(y_teste, y_pred_RL_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_RL_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_RL_padronizado, fp_RL_padronizado, fn_RL_padronizado, tp_RL_padronizado = conf_matrix_RL_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_RL_padronizado = roc_auc_score(y_teste, y_pred_proba_RL_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_RL_padronizado, tpr_RL_padronizado, thresholds_RL_padronizado = roc_curve(y_teste, y_pred_proba_RL_pos_padronizado)

# AUC em teste
auc_RL_padronizado = auc(fpr_RL_padronizado, tpr_RL_padronizado)

# Acurácia em teste
acuracia_RL_padronizado = accuracy_score(y_teste, y_pred_RL_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_RL_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_RL_padronizado:.4f}')
print(f'AUC Score: {auc_RL_padronizado:.4f}')





# Salvando as métricas em um DataFrame
modelo_RL_padronizado_df = pd.DataFrame({
    'Nome do Modelo': ['Modelo 1 - Padronizado'],
    'Nome do Algoritmo': ['Logistic Regression'],
    'ROC_AUC Score': [roc_auc_RL_padronizado],
    'AUC Score': [auc_RL_padronizado],
    'Acuracia': [acuracia_RL_padronizado]    
})

# Concatenando com as métricas anteriores
df_modelos = pd.concat([df_modelos, modelo_RL_padronizado_df], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)








from sklearn.preprocessing import PolynomialFeatures

# Criando interações de segundo grau entre variáveis contínuas
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_treino_interactions = poly.fit_transform(X_treino_padronizado[variaveis_continuas])
X_teste_interactions = poly.transform(X_teste_padronizado[variaveis_continuas])

# Convertendo de volta para DataFrame para facilidade de visualização e concatenação
X_treino_interactions = pd.DataFrame(X_treino_interactions, columns=poly.get_feature_names(variaveis_continuas))
X_teste_interactions = pd.DataFrame(X_teste_interactions, columns=poly.get_feature_names(variaveis_continuas))

# Concatenando as interações ao conjunto original
X_treino_final = pd.concat([X_treino_padronizado.reset_index(drop=True), X_treino_interactions], axis=1)
X_teste_final = pd.concat([X_teste_padronizado.reset_index(drop=True), X_teste_interactions], axis=1)

display(X_treino_final.sample(5))
display(X_teste_final.sample(5))


X_treino_final.columns





from sklearn.feature_selection import SelectKBest, f_classif
import seaborn as sns
import matplotlib.pyplot as plt

# Separando as variáveis de entrada (X) e a variável alvo (y) no conjunto de treino final
X = X_treino_final
y = y_treino

# Aplicando o SelectKBest para selecionar as melhores variáveis
k_best = SelectKBest(score_func=f_classif, k='all')
X_new = k_best.fit_transform(X, y)

# Visualizando as pontuações das variáveis
scores = k_best.scores_

# Criando um DataFrame com as pontuações
features = pd.DataFrame({
    'Feature': X.columns,
    'Score': scores
})

# Ordenando as variáveis pelas pontuações
features = features.sort_values(by='Score', ascending=False)

# Exibindo as pontuações
print(features)
print('\n-------------------------------------------------------------------------------------\n')

# Selecionando as 7 melhores variáveis, removendo duplicatas
selected_features = []
for feature in features['Feature']:
    if feature not in selected_features:
        selected_features.append(feature)
    if len(selected_features) == 7:
        break

print("\nMelhores variáveis:", selected_features)
print('\n-------------------------------------------------------------------------------------\n')

# Criando novos DataFrames apenas com as melhores variáveis
X_treino_selecionado = X_treino_final.loc[:, selected_features]
X_teste_selecionado = X_teste_final.loc[:, selected_features]

# Garantindo que não haja duplicatas
X_treino_selecionado = X_treino_selecionado.loc[:, ~X_treino_selecionado.columns.duplicated()]
X_teste_selecionado = X_teste_selecionado.loc[:, ~X_teste_selecionado.columns.duplicated()]

# Visualizando graficamente as pontuações das variáveis
plt.figure(figsize=(10, 6))
sns.barplot(x='Score', y='Feature', data=features, palette='viridis')
plt.title('Importância das Variáveis')
plt.xlabel('Score')
plt.ylabel('Variáveis')
plt.show()

# Verificando os DataFrames com as variáveis selecionadas
display(X_treino_selecionado.sample(5))
display(X_teste_selecionado.sample(5))

# Verificando as colunas
print(X_treino_selecionado.columns)
print(X_teste_selecionado.columns)


print(X_treino_final.columns)
print(X_teste_final.columns)
print(X_treino_selecionado.columns)
print(X_teste_selecionado.columns)





# Cria um dataframe para receber as métricas de cada modelo
df_modelos = pd.DataFrame()








# Configuração do pipeline de pré-processamento e modelo
pipeline = Pipeline([
    ('scaler', StandardScaler()),                    # Adiciona o escalonador
    ('logistic', LogisticRegression(max_iter=1000))  # Aumenta max_iter para garantir a convergência
])

# Define lista de hiperparâmetros
tuned_params_RL = {
    'logistic__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
    'logistic__penalty': ['l2']  # 'l1' está disponível apenas com o solucionador 'liblinear'
}

# Criando modelos com GridSearch
grid_search = GridSearchCV(pipeline, 
                         tuned_params_RL, 
                         scoring='roc_auc', 
                         n_jobs=-1)
grid_search.fit(X_treino_padronizado, y_treino)

# Visualizando o melhor modelo
print(grid_search.best_estimator_)





## Criando e Treinando o Modelo

# Obtendo os melhores hiperparâmetros encontrados
best_params = grid_search.best_params_

# Criando o Modelo
modelo_RL_padronizado = LogisticRegression(
    C=best_params['logistic__C'], 
    penalty='l2', 
    max_iter=1000
)

# Treinando o Modelo
print(modelo_RL_padronizado.fit(X_treino_padronizado, y_treino))

print('\n=================================================================================\n')


## Previsões

print('\nPrevisões\n\n')

# Previsões com dados de teste
X_teste_padronizado = X_teste.copy()
X_teste_padronizado[variaveis_continuas] = scaler.transform(X_teste_padronizado[variaveis_continuas])

y_pred_RL_padronizado = modelo_RL_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_RL_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_RL_padronizado = modelo_RL_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_RL_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_RL_pos_padronizado = modelo_RL_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_RL_pos_padronizado[:5])

print('\n\n=================================================================================\n')


## Avaliação do Modelo

print('\nAvaliação do Modelo\n\n')

# Matriz de confusão
conf_matrix_RL_padronizado = confusion_matrix(y_teste, y_pred_RL_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_RL_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_RL_padronizado, fp_RL_padronizado, fn_RL_padronizado, tp_RL_padronizado = conf_matrix_RL_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_RL_padronizado = roc_auc_score(y_teste, y_pred_proba_RL_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_RL_padronizado, tpr_RL_padronizado, thresholds_RL_padronizado = roc_curve(y_teste, y_pred_proba_RL_pos_padronizado)

# AUC em teste
auc_RL_padronizado = auc(fpr_RL_padronizado, tpr_RL_padronizado)

# Acurácia em teste
acuracia_RL_padronizado = accuracy_score(y_teste, y_pred_RL_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_RL_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_RL_padronizado:.4f}')
print(f'AUC Score: {auc_RL_padronizado:.4f}')





# Salvando as métricas em um DataFrame
modelo_RL_padronizado_df = pd.DataFrame({
    'Nome do Modelo': ['Modelo 1 - V1'],
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': ['Logistic Regression'],
    'ROC_AUC Score': [roc_auc_RL_padronizado],
    'AUC Score': [auc_RL_padronizado],
    'Acuracia': [acuracia_RL_padronizado]    
})

# Concatenando com as métricas anteriores
df_modelos = pd.concat([df_modelos, modelo_RL_padronizado_df], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





pipeline = Pipeline([
    ('scaler', StandardScaler()),                    
    ('logistic', LogisticRegression(max_iter=1000))  
])

# Define lista de hiperparâmetros
tuned_params_RL = {
    'logistic__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
    'logistic__penalty': ['l2']
}

# Criando modelos com GridSearch
grid_search = GridSearchCV(pipeline, 
                         tuned_params_RL, 
                         scoring='roc_auc', 
                         n_jobs=-1)
grid_search.fit(X_treino_final, y_treino)

# Visualizando o melhor modelo
print(grid_search.best_estimator_)





# Obtendo os melhores hiperparâmetros encontrados
best_params = grid_search.best_params_

# Criando o Modelo
modelo_RL_final = LogisticRegression(
    C=best_params['logistic__C'], 
    penalty='l2', 
    max_iter=1000
)

# Treinando o Modelo
modelo_RL_final.fit(X_treino_final, y_treino)

# Previsões com dados de teste
y_pred_RL_final = modelo_RL_final.predict(X_teste_final)
y_pred_proba_RL_pos_final = modelo_RL_final.predict_proba(X_teste_final)[:, 1]

# Avaliação do Modelo
conf_matrix_RL_final = confusion_matrix(y_teste, y_pred_RL_final)
roc_auc_RL_final = roc_auc_score(y_teste, y_pred_proba_RL_pos_final)
fpr_RL_final, tpr_RL_final, thresholds_RL_final = roc_curve(y_teste, y_pred_proba_RL_pos_final)
auc_RL_final = auc(fpr_RL_final, tpr_RL_final)
acuracia_RL_final = accuracy_score(y_teste, y_pred_RL_final)





# Salvando as métricas em um DataFrame
df_modelos_final = pd.DataFrame({
    'Nome do Modelo': ['Modelo 1 - V2'],
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': ['Logistic Regression'],
    'ROC_AUC Score': [roc_auc_RL_final],
    'AUC Score': [auc_RL_final],
    'Acuracia': [acuracia_RL_final]    
})

# Concatenando com as métricas anteriores
df_modelos = pd.concat([df_modelos, df_modelos_final], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Configuração do pipeline de pré-processamento e modelo
pipeline = Pipeline([
    ('scaler', StandardScaler()),                    
    ('logistic', LogisticRegression(max_iter=1000))  
])

# Define lista de hiperparâmetros
tuned_params_RL = {
    'logistic__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],
    'logistic__penalty': ['l2']
}

# Criando modelos com GridSearch
grid_search = GridSearchCV(pipeline, 
                         tuned_params_RL, 
                         scoring='roc_auc', 
                         n_jobs=-1)
grid_search.fit(X_treino_selecionado, y_treino)





# Obtendo os melhores hiperparâmetros encontrados
best_params = grid_search.best_params_

# Criando o Modelo
modelo_RL_selecionado = LogisticRegression(
    C=best_params['logistic__C'], 
    penalty='l2', 
    max_iter=1000
)

# Treinando o Modelo
modelo_RL_selecionado.fit(X_treino_selecionado, y_treino)

# Previsões com dados de teste
y_pred_RL_selecionado = modelo_RL_selecionado.predict(X_teste_selecionado)
y_pred_proba_RL_pos_selecionado = modelo_RL_selecionado.predict_proba(X_teste_selecionado)[:, 1]

# Avaliação do Modelo
conf_matrix_RL_selecionado = confusion_matrix(y_teste, y_pred_RL_selecionado)
roc_auc_RL_selecionado = roc_auc_score(y_teste, y_pred_proba_RL_pos_selecionado)
fpr_RL_selecionado, tpr_RL_selecionado, thresholds_RL_selecionado = roc_curve(y_teste, y_pred_proba_RL_pos_selecionado)
auc_RL_selecionado = auc(fpr_RL_selecionado, tpr_RL_selecionado)
acuracia_RL_selecionado = accuracy_score(y_teste, y_pred_RL_selecionado)





# Salvando as métricas em um DataFrame
df_modelos_selecionado = pd.DataFrame({
    'Nome do Modelo': ['Modelo 1 - V3'],
    'Tipo de Dado': 'Com Feature Selection',
    'Nome do Algoritmo': ['Logistic Regression'],
    'ROC_AUC Score': [roc_auc_RL_selecionado],
    'AUC Score': [auc_RL_selecionado],
    'Acuracia': [acuracia_RL_selecionado]    
})

# Concatenando com as métricas anteriores
df_modelos = pd.concat([df_modelos, df_modelos_selecionado], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Removendo os índices 2 do DataFrame df_modelos
df_modelos = df_modelos.drop([2])

# Resetando o índice do DataFrame
df_modelos = df_modelos.reset_index(drop=True)

# Exibindo o DataFrame atualizado
display(df_modelos)











## Construção e Treinamento do Modelo

# Cria o modelo com hiperparâmetros padrão
modelo_RF_padrao = RandomForestClassifier()

# Treinando o modelo final no conjunto de treino
modelo_RF_padrao.fit(X_treino_padronizado, y_treino)

## Previsões

# Previsões com dados de teste
y_pred_RF_padrao = modelo_RF_padrao.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_RF_padrao[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_RF_padrao = modelo_RF_padrao.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_RF_padrao[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_RF_pos_padrao = modelo_RF_padrao.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_RF_pos_padrao[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_RF_padrao = confusion_matrix(y_teste, y_pred_RF_padrao)
print('Matriz de Confusão:')
print(conf_matrix_RF_padrao)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_RF_padrao, fp_RF_padrao, fn_RF_padrao, tp_RF_padrao = conf_matrix_RF_padrao.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_RF_padrao = roc_auc_score(y_teste, y_pred_proba_RF_pos_padrao)

# Calcula a curva ROC com dados e previsões em teste
fpr_RF_padrao, tpr_RF_padrao, thresholds_RF_padrao = roc_curve(y_teste, y_pred_proba_RF_pos_padrao)

# AUC em teste
auc_RF_padrao = auc(fpr_RF_padrao, tpr_RF_padrao)

# Acurácia em teste
acuracia_RF_padrao = accuracy_score(y_teste, y_pred_RF_padrao)

# Exibindo as métricas
print(f'Acurácia: {acuracia_RF_padrao:.4f}')
print(f'ROC AUC Score: {roc_auc_RF_padrao:.4f}')
print(f'AUC Score: {auc_RF_padrao:.4f}')





# Extrair coeficientes do modelo
importancias = modelo_RF_padrao.feature_importances_

# Criar DataFrame para visualização
features = X_treino_padronizado.columns
df_importancias = pd.DataFrame({'Feature': features, 'Importance': importancias})
df_importancias = df_importancias.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('\n====== Visualizando Importância das Variáveis ======\n')
print(80 * '-')
print(df_importancias)
print(80 * '-')
print(80 * '-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias)
plt.title('Feature Importance from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





dict_modelo_RF_padrao = {
    'Nome do Modelo': 'Modelo 2 - V1 (Sem Ajuste de Hiperparametros)', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Random Forest', 
    'ROC_AUC Score': roc_auc_RF_padrao,
    'AUC Score': auc_RF_padrao,
    'Acuracia': acuracia_RF_padrao
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_RF_padrao])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











## Construção e Treinamento do Modelo

# Extraindo melhores parâmetros                   (Descomentar aqui e colocar como code bloco acima)
#best_params = best_rf.get_params()

# Recria o modelo com os melhores hiperparâmetros
#modelo_RF = RandomForestClassifier(
#    n_estimators=best_params['n_estimators'],
#    min_samples_split=best_params['min_samples_split'],
#    min_samples_leaf=best_params['min_samples_leaf']
#)


# Cria Modelo Com Hiperparametros padrão 

# Cria o modelo com hiperparâmetros padrão
modelo_RF = RandomForestClassifier()


# Treinando o modelo final no conjunto de treino
modelo_RF.fit(X_treino_final, y_treino)


## Previsões

# Previsões com dados de teste
y_pred_RF = modelo_RF.predict(X_teste_final)
print('Previsões de Classe:')
print(y_pred_RF[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_RF = modelo_RF.predict_proba(X_teste_final)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_RF[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_RF_pos = modelo_RF.predict_proba(X_teste_final)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_RF_pos[:5])
print('\n' + '-' * 80 + '\n')


## Avaliação do Modelo

# Matriz de confusão
conf_matrix_RF = confusion_matrix(y_teste, y_pred_RF)
print('Matriz de Confusão:')
print(conf_matrix_RF)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_RF, fp_RF, fn_RF, tp_RF = conf_matrix_RF.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_RF = roc_auc_score(y_teste, y_pred_proba_RF_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_teste, y_pred_proba_RF_pos)

# AUC em teste
auc_RF = auc(fpr_RF, tpr_RF)

# Acurácia em teste
acuracia_RF = accuracy_score(y_teste, y_pred_RF)

# Exibindo as métricas
print(f'Acurácia: {acuracia_RF:.4f}')
print(f'ROC AUC Score: {roc_auc_RF:.4f}')
print(f'AUC Score: {auc_RF:.4f}')





# Visualizando Importância das Variáveis

# Extrair coeficientes do modelo
importancias = modelo_RF.feature_importances_

# Criar DataFrame para visualização
features = X_treino_final.columns
df_importancias = pd.DataFrame({'Feature': features, 'Importance': importancias})
df_importancias = df_importancias.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('\n====== Visualizando Importância das Variáveis ======\n')
print(80 * '-')
print(df_importancias)
print(80 * '-')
print(80 * '-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias)
plt.title('Feature Importance from Random Forest')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_RF = {
    'Nome do Modelo': 'Modelo 2 - V2 (Sem Ajuste de Hiperparametros)',
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': 'Random Forest', 
    'ROC_AUC Score': roc_auc_RF,
    'AUC Score': auc_RF,
    'Acuracia': acuracia_RF
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_RF])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











# Criando e Treinando o Modelo
modelo_KNN = KNeighborsClassifier(n_neighbors=3)          # adicionar aqui valor de k

# Treinando o modelo
modelo_KNN.fit(X_treino_padronizado, y_treino)

# Previsões com dados de teste
y_pred_KNN = modelo_KNN.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_KNN[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_KNN = modelo_KNN.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_KNN[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_KNN_pos = modelo_KNN.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_KNN_pos[:5])
print('\n' + '-' * 80 + '\n')

# Avaliação do Modelo
# Matriz de confusão
conf_matrix_KNN = confusion_matrix(y_teste, y_pred_KNN)
print('Matriz de Confusão:')
print(conf_matrix_KNN)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_KNN, fp_KNN, fn_KNN, tp_KNN = conf_matrix_KNN.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_KNN = roc_auc_score(y_teste, y_pred_proba_KNN_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_teste, y_pred_proba_KNN_pos)

# AUC em teste
auc_KNN = auc(fpr_KNN, tpr_KNN)

# Acurácia em teste
acuracia_KNN = accuracy_score(y_teste, y_pred_KNN)

# Exibindo as métricas
print(f'Acurácia: {acuracia_KNN:.4f}')
print(f'ROC AUC Score: {roc_auc_KNN:.4f}')
print(f'AUC Score: {auc_KNN:.4f}')





# Salvando as Métricas
dict_modelo_KNN_padronizado = {
    'Nome do Modelo': 'Modelo 3 - V1',
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'KNN',
    'ROC_AUC Score': roc_auc_KNN,
    'AUC Score': auc_KNN,
    'Acuracia': acuracia_KNN
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_KNN_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











# Criando e Treinando o Modelo
modelo_KNN = KNeighborsClassifier(n_neighbors=5)

# Treinando o modelo
modelo_KNN.fit(X_treino_final, y_treino)

# Previsões com dados de teste
y_pred_KNN = modelo_KNN.predict(X_teste_final)
print('Previsões de Classe:')
print(y_pred_KNN[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_KNN = modelo_KNN.predict_proba(X_teste_final)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_KNN[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_KNN_pos = modelo_KNN.predict_proba(X_teste_final)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_KNN_pos[:5])
print('\n' + '-' * 80 + '\n')

# Avaliação do Modelo
# Matriz de confusão
conf_matrix_KNN = confusion_matrix(y_teste, y_pred_KNN)
print('Matriz de Confusão:')
print(conf_matrix_KNN)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_KNN, fp_KNN, fn_KNN, tp_KNN = conf_matrix_KNN.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_KNN = roc_auc_score(y_teste, y_pred_proba_KNN_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_teste, y_pred_proba_KNN_pos)

# AUC em teste
auc_KNN = auc(fpr_KNN, tpr_KNN)

# Acurácia em teste
acuracia_KNN = accuracy_score(y_teste, y_pred_KNN)

# Exibindo as métricas
print(f'Acurácia: {acuracia_KNN:.4f}')
print(f'ROC AUC Score: {roc_auc_KNN:.4f}')
print(f'AUC Score: {auc_KNN:.4f}')





# Salvando as Métricas
dict_modelo_KNN_final = {
    'Nome do Modelo': 'Modelo 3 - V2',
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': 'KNN',
    'ROC_AUC Score': roc_auc_KNN,
    'AUC Score': auc_KNN,
    'Acuracia': acuracia_KNN
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_KNN_final])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Hiperparâmetros
tuned_params_DT = {
    'min_samples_split': [2, 3, 4, 5, 7], 
    'min_samples_leaf': [1, 2, 3, 4, 6], 
    'max_depth': [2, 3, 4, 5, 6, 7]
}

# Cria o modelo com RandomizedSearchCV
modelo = RandomizedSearchCV(DecisionTreeClassifier(), 
                            tuned_params_DT, 
                            n_iter=15, 
                            scoring='roc_auc', 
                            n_jobs=-1)

# Treinamento
modelo.fit(X_treino_padronizado, y_treino)

# Visualiza o melhor modelo
best_dt = modelo.best_estimator_
print(best_dt)





# Construindo e Treinando o Modelo com DecisionTreeClassifier
# Recriando o modelo com os melhores hiperparâmetros automaticamente
best_params = best_dt.get_params()

# Treinando o modelo final no conjunto de treino
modelo_DT = DecisionTreeClassifier(**best_params)
modelo_DT.fit(X_treino_padronizado, y_treino)

## Previsões
# Previsões com dados de teste
y_pred_DT = modelo_DT.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_DT[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_DT = modelo_DT.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_DT[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_DT_pos = modelo_DT.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_DT_pos[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo
# Matriz de confusão
conf_matrix_DT = confusion_matrix(y_teste, y_pred_DT)
print('Matriz de Confusão:')
print(conf_matrix_DT)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_DT, fp_DT, fn_DT, tp_DT = conf_matrix_DT.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_DT = roc_auc_score(y_teste, y_pred_proba_DT_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_DT, tpr_DT, thresholds_DT = roc_curve(y_teste, y_pred_proba_DT_pos)

# AUC em teste
auc_DT = auc(fpr_DT, tpr_DT)

# Acurácia em teste
acuracia_DT = accuracy_score(y_teste, y_pred_DT)

# Exibindo as métricas
print(f'Acurácia: {acuracia_DT:.4f}')
print(f'ROC AUC Score: {roc_auc_DT:.4f}')
print(f'AUC Score: {auc_DT:.4f}')





# Visualizando Importância das Variáveis
# Extrair coeficientes do modelo
importancias = modelo_DT.feature_importances_

# Criar DataFrame para visualização
features = X_treino_padronizado.columns
df_importancias = pd.DataFrame({'Feature': features, 'Importance': importancias})
df_importancias = df_importancias.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80 * '-')
print(df_importancias.head())
print(80 * '-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias)
plt.title('Feature Importance from Decision Tree')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_DT_padronizado = {
    'Nome do Modelo': 'Modelo 4 - V1',
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Decision Tree', 
    'ROC_AUC Score': roc_auc_DT,
    'AUC Score': auc_DT,
    'Acuracia': acuracia_DT
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_DT_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Hiperparâmetros
tuned_params_DT = {
    'min_samples_split': [2, 3, 4, 5, 7], 
    'min_samples_leaf': [1, 2, 3, 4, 6], 
    'max_depth': [2, 3, 4, 5, 6, 7]
}

# Cria o modelo com RandomizedSearchCV
modelo = RandomizedSearchCV(DecisionTreeClassifier(), 
                            tuned_params_DT, 
                            n_iter=15, 
                            scoring='roc_auc', 
                            n_jobs=-1)

# Treinamento
modelo.fit(X_treino_final, y_treino)

# Visualiza o melhor modelo
best_dt = modelo.best_estimator_
print(best_dt)





# Construindo e Treinando o Modelo com DecisionTreeClassifier
# Recriando o modelo com os melhores hiperparâmetros automaticamente
best_params = best_dt.get_params()

# Treinando o modelo final no conjunto de treino
modelo_DT = DecisionTreeClassifier(**best_params)
modelo_DT.fit(X_treino_final, y_treino)

## Previsões
# Previsões com dados de teste
y_pred_DT = modelo_DT.predict(X_teste_final)
print('Previsões de Classe:')
print(y_pred_DT[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_DT = modelo_DT.predict_proba(X_teste_final)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_DT[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_DT_pos = modelo_DT.predict_proba(X_teste_final)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_DT_pos[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo
# Matriz de confusão
conf_matrix_DT = confusion_matrix(y_teste, y_pred_DT)
print('Matriz de Confusão:')
print(conf_matrix_DT)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_DT, fp_DT, fn_DT, tp_DT = conf_matrix_DT.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_DT = roc_auc_score(y_teste, y_pred_proba_DT_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_DT, tpr_DT, thresholds_DT = roc_curve(y_teste, y_pred_proba_DT_pos)

# AUC em teste
auc_DT = auc(fpr_DT, tpr_DT)

# Acurácia em teste
acuracia_DT = accuracy_score(y_teste, y_pred_DT)

# Exibindo as métricas
print(f'Acurácia: {acuracia_DT:.4f}')
print(f'ROC AUC Score: {roc_auc_DT:.4f}')
print(f'AUC Score: {auc_DT:.4f}')





# Visualizando Importância das Variáveis
# Extrair coeficientes do modelo
importancias = modelo_DT.feature_importances_

# Criar DataFrame para visualização
features = X_treino_final.columns
df_importancias = pd.DataFrame({'Feature': features, 'Importance': importancias})
df_importancias = df_importancias.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80 * '-')
print(df_importancias.head())
print(80 * '-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias)
plt.title('Feature Importance from Decision Tree')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_DT_final = {
    'Nome do Modelo': 'Modelo 4 - V2',
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': 'Decision Tree', 
    'ROC_AUC Score': roc_auc_DT,
    'AUC Score': auc_DT,
    'Acuracia': acuracia_DT
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_DT_final])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











# Cria o modelo com hiperparâmetros padrão
modelo_SVM_padronizado = SVC(kernel='rbf', probability=True)

# Treinamento
modelo_SVM_padronizado.fit(X_treino_padronizado, y_treino)

## Previsões
# Previsões com dados de teste
y_pred_SVM_padronizado = modelo_SVM_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_SVM_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_SVM_padronizado = modelo_SVM_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_SVM_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_SVM_pos_padronizado = modelo_SVM_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_SVM_pos_padronizado[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo
# Matriz de confusão
conf_matrix_SVM_padronizado = confusion_matrix(y_teste, y_pred_SVM_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_SVM_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_SVM_padronizado, fp_SVM_padronizado, fn_SVM_padronizado, tp_SVM_padronizado = conf_matrix_SVM_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_SVM_padronizado = roc_auc_score(y_teste, y_pred_proba_SVM_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_SVM_padronizado, tpr_SVM_padronizado, thresholds_SVM_padronizado = roc_curve(y_teste, y_pred_proba_SVM_pos_padronizado)

# AUC em teste
auc_SVM_padronizado = auc(fpr_SVM_padronizado, tpr_SVM_padronizado)

# Acurácia em teste
acuracia_SVM_padronizado = accuracy_score(y_teste, y_pred_SVM_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_SVM_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_SVM_padronizado:.4f}')
print(f'AUC Score: {auc_SVM_padronizado:.4f}')





# Salvando as Métricas
dict_modelo_SVM_padronizado = {
    'Nome do Modelo': 'Modelo 5 - V1',
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'SVM',
    'ROC_AUC Score': roc_auc_SVM_padronizado,
    'AUC Score': auc_SVM_padronizado,
    'Acuracia': acuracia_SVM_padronizado
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_SVM_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











# Versão 2 - Dados Com Eng Features
# Cria o modelo com hiperparâmetros padrão
modelo_SVM_final = SVC(kernel='rbf', probability=True)

# Treinamento
modelo_SVM_final.fit(X_treino_final, y_treino)

## Previsões
# Previsões com dados de teste
y_pred_SVM_final = modelo_SVM_final.predict(X_teste_final)
print('Previsões de Classe:')
print(y_pred_SVM_final[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_SVM_final = modelo_SVM_final.predict_proba(X_teste_final)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_SVM_final[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_SVM_pos_final = modelo_SVM_final.predict_proba(X_teste_final)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_SVM_pos_final[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo
# Matriz de confusão
conf_matrix_SVM_final = confusion_matrix(y_teste, y_pred_SVM_final)
print('Matriz de Confusão:')
print(conf_matrix_SVM_final)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_SVM_final, fp_SVM_final, fn_SVM_final, tp_SVM_final = conf_matrix_SVM_final.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_SVM_final = roc_auc_score(y_teste, y_pred_proba_SVM_pos_final)

# Calcula a curva ROC com dados e previsões em teste
fpr_SVM_final, tpr_SVM_final, thresholds_SVM_final = roc_curve(y_teste, y_pred_proba_SVM_pos_final)

# AUC em teste
auc_SVM_final = auc(fpr_SVM_final, tpr_SVM_final)

# Acurácia em teste
acuracia_SVM_final = accuracy_score(y_teste, y_pred_SVM_final)

# Exibindo as métricas
print(f'Acurácia: {acuracia_SVM_final:.4f}')
print(f'ROC AUC Score: {roc_auc_SVM_final:.4f}')
print(f'AUC Score: {auc_SVM_final:.4f}')





# Salvando as Métricas - Versão 2
dict_modelo_SVM_final = {
    'Nome do Modelo': 'Modelo 5 - V2',
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': 'SVM',
    'ROC_AUC Score': roc_auc_SVM_final,
    'AUC Score': auc_SVM_final,
    'Acuracia': acuracia_SVM_final
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_SVM_final])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Hiperparâmetros
tuned_params_GBM = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3]
}

# Cria o modelo com RandomizedSearchCV
modelo = RandomizedSearchCV(GradientBoostingClassifier(), 
                            tuned_params_GBM, 
                            n_iter=15, 
                            scoring='roc_auc', 
                            n_jobs=-1)

# Treinamento
modelo.fit(X_treino_padronizado, y_treino)

# Visualiza o melhor modelo
best_gbm = modelo.best_estimator_
print(best_gbm)





# Recriando o modelo com os melhores hiperparâmetros automaticamente
best_params = best_gbm.get_params()

# Treinando o modelo final no conjunto de treino
modelo_GBM_padronizado = GradientBoostingClassifier(**best_params)
modelo_GBM_padronizado.fit(X_treino_padronizado, y_treino)
print(modelo_GBM_padronizado)

## Previsões

# Previsões com dados de teste
y_pred_GBM_padronizado = modelo_GBM_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_GBM_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_GBM_padronizado = modelo_GBM_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_GBM_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_GBM_pos_padronizado = modelo_GBM_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_GBM_pos_padronizado[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_GBM_padronizado = confusion_matrix(y_teste, y_pred_GBM_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_GBM_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_GBM_padronizado, fp_GBM_padronizado, fn_GBM_padronizado, tp_GBM_padronizado = conf_matrix_GBM_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_GBM_padronizado = roc_auc_score(y_teste, y_pred_proba_GBM_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_GBM_padronizado, tpr_GBM_padronizado, thresholds_GBM_padronizado = roc_curve(y_teste, y_pred_proba_GBM_pos_padronizado)

# AUC em teste
auc_GBM_padronizado = auc(fpr_GBM_padronizado, tpr_GBM_padronizado)

# Acurácia em teste
acuracia_GBM_padronizado = accuracy_score(y_teste, y_pred_GBM_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_GBM_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_GBM_padronizado:.4f}')
print(f'AUC Score: {auc_GBM_padronizado:.4f}')





# Extrair coeficientes do modelo
importancias_padronizado = modelo_GBM_padronizado.feature_importances_

# Criar DataFrame para visualização
features_padronizado = X_treino_padronizado.columns
df_importancias_padronizado = pd.DataFrame({'Feature': features_padronizado, 'Importance': importancias_padronizado})
df_importancias_padronizado = df_importancias_padronizado.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_padronizado.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_padronizado)
plt.title('Feature Importance from Gradient Boosting Machine - Dados Padronizados')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_GBM_padronizado = {
    'Nome do Modelo': 'Modelo 6 - V1', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Gradient Boosting Machine', 
    'ROC_AUC Score': roc_auc_GBM_padronizado,
    'AUC Score': auc_GBM_padronizado,
    'Acuracia': acuracia_GBM_padronizado
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_GBM_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Hiperparâmetros
tuned_params_GBM = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3]
}

# Cria o modelo com RandomizedSearchCV
modelo = RandomizedSearchCV(GradientBoostingClassifier(), 
                            tuned_params_GBM, 
                            n_iter=15, 
                            scoring='roc_auc', 
                            n_jobs=-1)

# Treinamento
modelo.fit(X_treino_final, y_treino)

# Visualiza o melhor modelo
best_gbm = modelo.best_estimator_
print(best_gbm)





# Recriando o modelo com os melhores hiperparâmetros automaticamente
best_params = best_gbm.get_params()

# Treinando o modelo final no conjunto de treino
modelo_GBM_final = GradientBoostingClassifier(**best_params)
modelo_GBM_final.fit(X_treino_final, y_treino)
print(modelo_GBM_final)

## Previsões

# Previsões com dados de teste
y_pred_GBM_final = modelo_GBM_final.predict(X_teste_final)
print('Previsões de Classe:')
print(y_pred_GBM_final[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_GBM_final = modelo_GBM_final.predict_proba(X_teste_final)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_GBM_final[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_GBM_pos_final = modelo_GBM_final.predict_proba(X_teste_final)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_GBM_pos_final[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_GBM_final = confusion_matrix(y_teste, y_pred_GBM_final)
print('Matriz de Confusão:')
print(conf_matrix_GBM_final)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_GBM_final, fp_GBM_final, fn_GBM_final, tp_GBM_final = conf_matrix_GBM_final.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_GBM_final = roc_auc_score(y_teste, y_pred_proba_GBM_pos_final)

# Calcula a curva ROC com dados e previsões em teste
fpr_GBM_final, tpr_GBM_final, thresholds_GBM_final = roc_curve(y_teste, y_pred_proba_GBM_pos_final)

# AUC em teste
auc_GBM_final = auc(fpr_GBM_final, tpr_GBM_final)

# Acurácia em teste
acuracia_GBM_final = accuracy_score(y_teste, y_pred_GBM_final)

# Exibindo as métricas
print(f'Acurácia: {acuracia_GBM_final:.4f}')
print(f'ROC AUC Score: {roc_auc_GBM_final:.4f}')
print(f'AUC Score: {auc_GBM_final:.4f}')





# Extrair coeficientes do modelo
importancias_final = modelo_GBM_final.feature_importances_

# Criar DataFrame para visualização
features_final = X_treino_final.columns
df_importancias_final = pd.DataFrame({'Feature': features_final, 'Importance': importancias_final})
df_importancias_final = df_importancias_final.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_final.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_final)
plt.title('Feature Importance from Gradient Boosting Machine - Dados Com Eng Features')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_GBM_final = {
    'Nome do Modelo': 'Modelo 6 - V2', 
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': 'Gradient Boosting Machine', 
    'ROC_AUC Score': roc_auc_GBM_final,
    'AUC Score': auc_GBM_final,
    'Acuracia': acuracia_GBM_final
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_GBM_final])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Hiperparâmetros
tuned_params_AB = {
    'n_estimators': [50, 100, 150, 200, 250],
    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]
}

# Cria o modelo com RandomizedSearchCV
modelo = RandomizedSearchCV(AdaBoostClassifier(), 
                            tuned_params_AB, 
                            n_iter=10, 
                            scoring='roc_auc', 
                            n_jobs=-1)

# Treinamento
modelo.fit(X_treino_padronizado, y_treino)

# Visualiza o melhor modelo
best_ab = modelo.best_estimator_
print(best_ab)





# Recriando o modelo com os melhores hiperparâmetros automaticamente
best_params = best_ab.get_params()

# Treinando o modelo final no conjunto de treino
modelo_AB_padronizado = AdaBoostClassifier(**best_params)
modelo_AB_padronizado.fit(X_treino_padronizado, y_treino)
print(modelo_AB_padronizado)

## Previsões

# Previsões com dados de teste
y_pred_AB_padronizado = modelo_AB_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_AB_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_AB_padronizado = modelo_AB_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_AB_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_AB_pos_padronizado = modelo_AB_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_AB_pos_padronizado[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_AB_padronizado = confusion_matrix(y_teste, y_pred_AB_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_AB_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_AB_padronizado, fp_AB_padronizado, fn_AB_padronizado, tp_AB_padronizado = conf_matrix_AB_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_AB_padronizado = roc_auc_score(y_teste, y_pred_proba_AB_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_AB_padronizado, tpr_AB_padronizado, thresholds_AB_padronizado = roc_curve(y_teste, y_pred_proba_AB_pos_padronizado)

# AUC em teste
auc_AB_padronizado = auc(fpr_AB_padronizado, tpr_AB_padronizado)

# Acurácia em teste
acuracia_AB_padronizado = accuracy_score(y_teste, y_pred_AB_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_AB_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_AB_padronizado:.4f}')
print(f'AUC Score: {auc_AB_padronizado:.4f}')





# Extrair coeficientes do modelo
importancias_padronizado = modelo_AB_padronizado.feature_importances_

# Criar DataFrame para visualização
features_padronizado = X_treino_padronizado.columns
df_importancias_padronizado = pd.DataFrame({'Feature': features_padronizado, 'Importance': importancias_padronizado})
df_importancias_padronizado = df_importancias_padronizado.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_padronizado.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_padronizado)
plt.title('Feature Importance from AdaBoost - Dados Padronizados')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_AB_padronizado = {
    'Nome do Modelo': 'Modelo 7 - V1', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'AdaBoost', 
    'ROC_AUC Score': roc_auc_AB_padronizado,
    'AUC Score': auc_AB_padronizado,
    'Acuracia': acuracia_AB_padronizado
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_AB_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Hiperparâmetros
tuned_params_AB = {
    'n_estimators': [50, 100, 150, 200, 250],
    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]
}

# Cria o modelo com RandomizedSearchCV
modelo = RandomizedSearchCV(AdaBoostClassifier(), 
                            tuned_params_AB, 
                            n_iter=10, 
                            scoring='roc_auc', 
                            n_jobs=-1)

# Treinamento
modelo.fit(X_treino_final, y_treino)

# Visualiza o melhor modelo
best_ab = modelo.best_estimator_
print(best_ab)





# Recriando o modelo com os melhores hiperparâmetros automaticamente
best_params = best_ab.get_params()

# Treinando o modelo final no conjunto de treino
modelo_AB_final = AdaBoostClassifier(**best_params)
modelo_AB_final.fit(X_treino_final, y_treino)
print(modelo_AB_final)

## Previsões

# Previsões com dados de teste
y_pred_AB_final = modelo_AB_final.predict(X_teste_final)
print('Previsões de Classe:')
print(y_pred_AB_final[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_AB_final = modelo_AB_final.predict_proba(X_teste_final)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_AB_final[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_AB_pos_final = modelo_AB_final.predict_proba(X_teste_final)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_AB_pos_final[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_AB_final = confusion_matrix(y_teste, y_pred_AB_final)
print('Matriz de Confusão:')
print(conf_matrix_AB_final)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_AB_final, fp_AB_final, fn_AB_final, tp_AB_final = conf_matrix_AB_final.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_AB_final = roc_auc_score(y_teste, y_pred_proba_AB_pos_final)

# Calcula a curva ROC com dados e previsões em teste
fpr_AB_final, tpr_AB_final, thresholds_AB_final = roc_curve(y_teste, y_pred_proba_AB_pos_final)

# AUC em teste
auc_AB_final = auc(fpr_AB_final, tpr_AB_final)

# Acurácia em teste
acuracia_AB_final = accuracy_score(y_teste, y_pred_AB_final)

# Exibindo as métricas
print(f'Acurácia: {acuracia_AB_final:.4f}')
print(f'ROC AUC Score: {roc_auc_AB_final:.4f}')
print(f'AUC Score: {auc_AB_final:.4f}')





# Extrair coeficientes do modelo
importancias_final = modelo_AB_final.feature_importances_

# Criar DataFrame para visualização
features_final = X_treino_final.columns
df_importancias_final = pd.DataFrame({'Feature': features_final, 'Importance': importancias_final})
df_importancias_final = df_importancias_final.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_final.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_final)
plt.title('Feature Importance from AdaBoost - Dados Com Eng Features')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_AB_final = {
    'Nome do Modelo': 'Modelo 7 - V2', 
    'Tipo de Dado': 'Com Eng Features',
    'Nome do Algoritmo': 'AdaBoost', 
    'ROC_AUC Score': roc_auc_AB_final,
    'AUC Score': auc_AB_final,
    'Acuracia': acuracia_AB_final
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_AB_final])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











# Criando o modelo com hiperparâmetros padrão
modelo_LGBM_padronizado = LGBMClassifier()

# Treinamento do modelo
modelo_LGBM_padronizado.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_LGBM_padronizado)

## Previsões

# Previsões com dados de teste
y_pred_LGBM_padronizado = modelo_LGBM_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_LGBM_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_LGBM_padronizado = modelo_LGBM_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_LGBM_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_LGBM_pos_padronizado = modelo_LGBM_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_LGBM_pos_padronizado[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_LGBM_padronizado = confusion_matrix(y_teste, y_pred_LGBM_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_LGBM_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_LGBM_padronizado, fp_LGBM_padronizado, fn_LGBM_padronizado, tp_LGBM_padronizado = conf_matrix_LGBM_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_LGBM_padronizado = roc_auc_score(y_teste, y_pred_proba_LGBM_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_LGBM_padronizado, tpr_LGBM_padronizado, thresholds_LGBM_padronizado = roc_curve(y_teste, y_pred_proba_LGBM_pos_padronizado)

# AUC em teste
auc_LGBM_padronizado = auc(fpr_LGBM_padronizado, tpr_LGBM_padronizado)

# Acurácia em teste
acuracia_LGBM_padronizado = accuracy_score(y_teste, y_pred_LGBM_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_LGBM_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_LGBM_padronizado:.4f}')
print(f'AUC Score: {auc_LGBM_padronizado:.4f}')





# Extrair coeficientes do modelo
importancias_padronizado = modelo_LGBM_padronizado.feature_importances_

# Criar DataFrame para visualização
features_padronizado = X_treino_padronizado.columns
df_importancias_padronizado = pd.DataFrame({'Feature': features_padronizado, 'Importance': importancias_padronizado})
df_importancias_padronizado = df_importancias_padronizado.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_padronizado.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_padronizado)
plt.title('Feature Importance from LightGBM - Dados Padronizados')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_LGBM_padronizado = {
    'Nome do Modelo': 'Modelo 8 - V1 (Sem Ajuste de Hiperparâmetros)', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'LightGBM', 
    'ROC_AUC Score': roc_auc_LGBM_padronizado,
    'AUC Score': auc_LGBM_padronizado,
    'Acuracia': acuracia_LGBM_padronizado
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_LGBM_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)



































# Criando o modelo com hiperparâmetros padrão
modelo_XGB_padronizado = XGBClassifier()

# Treinamento do modelo
modelo_XGB_padronizado.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_XGB_padronizado)

## Previsões

# Previsões com dados de teste
y_pred_XGB_padronizado = modelo_XGB_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_XGB_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_XGB_padronizado = modelo_XGB_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_XGB_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_XGB_pos_padronizado = modelo_XGB_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_XGB_pos_padronizado[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_XGB_padronizado = confusion_matrix(y_teste, y_pred_XGB_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_XGB_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_XGB_padronizado, fp_XGB_padronizado, fn_XGB_padronizado, tp_XGB_padronizado = conf_matrix_XGB_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_XGB_padronizado = roc_auc_score(y_teste, y_pred_proba_XGB_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_XGB_padronizado, tpr_XGB_padronizado, thresholds_XGB_padronizado = roc_curve(y_teste, y_pred_proba_XGB_pos_padronizado)

# AUC em teste
auc_XGB_padronizado = auc(fpr_XGB_padronizado, tpr_XGB_padronizado)

# Acurácia em teste
acuracia_XGB_padronizado = accuracy_score(y_teste, y_pred_XGB_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_XGB_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_XGB_padronizado:.4f}')
print(f'AUC Score: {auc_XGB_padronizado:.4f}')





# Extrair coeficientes do modelo
importancias_padronizado = modelo_XGB_padronizado.feature_importances_

# Criar DataFrame para visualização
features_padronizado = X_treino_padronizado.columns
df_importancias_padronizado = pd.DataFrame({'Feature': features_padronizado, 'Importance': importancias_padronizado})
df_importancias_padronizado = df_importancias_padronizado.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_padronizado.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_padronizado)
plt.title('Feature Importance from XGBoost - Dados Padronizados')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_XGB_padronizado = {
    'Nome do Modelo': 'Modelo 9 - V1 (Sem Ajuste de Hiperparâmetros)', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'XGBoost', 
    'ROC_AUC Score': roc_auc_XGB_padronizado,
    'AUC Score': auc_XGB_padronizado,
    'Acuracia': acuracia_XGB_padronizado
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_XGB_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)



































# Versão 1 - Dados Padronizados

# Criando o modelo com hiperparâmetros padrão
modelo_NB_padronizado = GaussianNB()

# Treinamento do modelo
modelo_NB_padronizado.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_NB_padronizado)

## Previsões

# Previsões com dados de teste
y_pred_NB_padronizado = modelo_NB_padronizado.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_NB_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_NB_padronizado = modelo_NB_padronizado.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_NB_padronizado[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_NB_pos_padronizado = modelo_NB_padronizado.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_NB_pos_padronizado[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_NB_padronizado = confusion_matrix(y_teste, y_pred_NB_padronizado)
print('Matriz de Confusão:')
print(conf_matrix_NB_padronizado)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_NB_padronizado, fp_NB_padronizado, fn_NB_padronizado, tp_NB_padronizado = conf_matrix_NB_padronizado.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_NB_padronizado = roc_auc_score(y_teste, y_pred_proba_NB_pos_padronizado)

# Calcula a curva ROC com dados e previsões em teste
fpr_NB_padronizado, tpr_NB_padronizado, thresholds_NB_padronizado = roc_curve(y_teste, y_pred_proba_NB_pos_padronizado)

# AUC em teste
auc_NB_padronizado = auc(fpr_NB_padronizado, tpr_NB_padronizado)

# Acurácia em teste
acuracia_NB_padronizado = accuracy_score(y_teste, y_pred_NB_padronizado)

# Exibindo as métricas
print(f'Acurácia: {acuracia_NB_padronizado:.4f}')
print(f'ROC AUC Score: {roc_auc_NB_padronizado:.4f}')
print(f'AUC Score: {auc_NB_padronizado:.4f}')





# Naive Bayes não fornece uma maneira direta de calcular a importância das variáveis,
# mas podemos usar a média das variâncias para ter uma ideia da importância das variáveis.

# Extraindo a variância das variáveis do modelo
importancias_padronizado = modelo_NB_padronizado.theta_.var(axis=0)

# Criar DataFrame para visualização
features_padronizado = X_treino_padronizado.columns
df_importancias_padronizado = pd.DataFrame({'Feature': features_padronizado, 'Importance': importancias_padronizado})
df_importancias_padronizado = df_importancias_padronizado.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias_padronizado.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias_padronizado)
plt.title('Feature Importance from Naive Bayes - Dados Padronizados')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()





# Salvando as Métricas
dict_modelo_NB_padronizado = {
    'Nome do Modelo': 'Modelo 10 - V1 (Sem Ajuste de Hiperparâmetros)', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Naive Bayes', 
    'ROC_AUC Score': roc_auc_NB_padronizado,
    'AUC Score': auc_NB_padronizado,
    'Acuracia': acuracia_NB_padronizado
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_NB_padronizado])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)



































from sklearn.neural_network import MLPClassifier

# Criando o modelo com hiperparâmetros padrão
modelo_NN = MLPClassifier(max_iter=1000, random_state=42)

# Treinamento do modelo
modelo_NN.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_NN)

## Previsões

# Previsões com dados de teste
y_pred_NN = modelo_NN.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_NN[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_NN = modelo_NN.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_NN[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_NN_pos = modelo_NN.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_NN_pos[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_NN = confusion_matrix(y_teste, y_pred_NN)
print('Matriz de Confusão:')
print(conf_matrix_NN)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_NN, fp_NN, fn_NN, tp_NN = conf_matrix_NN.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_NN = roc_auc_score(y_teste, y_pred_proba_NN_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_NN, tpr_NN, thresholds_NN = roc_curve(y_teste, y_pred_proba_NN_pos)

# AUC em teste
auc_NN = auc(fpr_NN, tpr_NN)

# Acurácia em teste
acuracia_NN = accuracy_score(y_teste, y_pred_NN)

# Exibindo as métricas
print(f'Acurácia: {acuracia_NN:.4f}')
print(f'ROC AUC Score: {roc_auc_NN:.4f}')
print(f'AUC Score: {auc_NN:.4f}')

# Salvando as Métricas
dict_modelo_NN = {
    'Nome do Modelo': 'Modelo 11 (Sem Ajuste de Hiperparâmetros)',
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Neural Networks', 
    'ROC_AUC Score': roc_auc_NN,
    'AUC Score': auc_NN,
    'Acuracia': acuracia_NN
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_NN])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)















































from catboost import CatBoostClassifier

# Criando o modelo com hiperparâmetros padrão
modelo_CatBoost = CatBoostClassifier(verbose=0, random_state=42)

# Treinamento do modelo
modelo_CatBoost.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_CatBoost)

## Previsões

# Previsões com dados de teste
y_pred_CatBoost = modelo_CatBoost.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_CatBoost[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_CatBoost = modelo_CatBoost.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_CatBoost[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_CatBoost_pos = modelo_CatBoost.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_CatBoost_pos[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_CatBoost = confusion_matrix(y_teste, y_pred_CatBoost)
print('Matriz de Confusão:')
print(conf_matrix_CatBoost)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_CatBoost, fp_CatBoost, fn_CatBoost, tp_CatBoost = conf_matrix_CatBoost.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_CatBoost = roc_auc_score(y_teste, y_pred_proba_CatBoost_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_CatBoost, tpr_CatBoost, thresholds_CatBoost = roc_curve(y_teste, y_pred_proba_CatBoost_pos)

# AUC em teste
auc_CatBoost = auc(fpr_CatBoost, tpr_CatBoost)

# Acurácia em teste
acuracia_CatBoost = accuracy_score(y_teste, y_pred_CatBoost)

# Exibindo as métricas
print(f'Acurácia: {acuracia_CatBoost:.4f}')
print(f'ROC AUC Score: {roc_auc_CatBoost:.4f}')
print(f'AUC Score: {auc_CatBoost:.4f}')

# Salvando as Métricas
dict_modelo_CatBoost = {
    'Nome do Modelo': 'Modelo 12 - V1 (Sem AJuste de Hiperparâmetros)',
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'CatBoost', 
    'ROC_AUC Score': roc_auc_CatBoost,
    'AUC Score': auc_CatBoost,
    'Acuracia': acuracia_CatBoost
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_CatBoost])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)


<br>

#### Visualizando Importância das Variáveis


# Extrair coeficientes do modelo
importancias = modelo_CatBoost.get_feature_importance()

# Criar DataFrame para visualização
features = X_treino_padronizado.columns
df_importancias = pd.DataFrame({'Feature': features, 'Importance': importancias})
df_importancias = df_importancias.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias.head(10))
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias)
plt.title('Feature Importance from CatBoost')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()











from sklearn.ensemble import ExtraTreesClassifier

# Criando o modelo com hiperparâmetros padrão
modelo_ET = ExtraTreesClassifier(random_state=42)

# Treinamento do modelo
modelo_ET.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_ET)

## Previsões

# Previsões com dados de teste
y_pred_ET = modelo_ET.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_ET[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_ET = modelo_ET.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_ET[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_ET_pos = modelo_ET.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_ET_pos[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_ET = confusion_matrix(y_teste, y_pred_ET)
print('Matriz de Confusão:')
print(conf_matrix_ET)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_ET, fp_ET, fn_ET, tp_ET = conf_matrix_ET.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_ET = roc_auc_score(y_teste, y_pred_proba_ET_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_ET, tpr_ET, thresholds_ET = roc_curve(y_teste, y_pred_proba_ET_pos)

# AUC em teste
auc_ET = auc(fpr_ET, tpr_ET)

# Acurácia em teste
acuracia_ET = accuracy_score(y_teste, y_pred_ET)

# Exibindo as métricas
print(f'Acurácia: {acuracia_ET:.4f}')
print(f'ROC AUC Score: {roc_auc_ET:.4f}')
print(f'AUC Score: {auc_ET:.4f}')

# Extrair coeficientes do modelo
importancias = modelo_ET.feature_importances_

# Criar DataFrame para visualização
features = X_treino_padronizado.columns
df_importancias = pd.DataFrame({'Feature': features, 'Importance': importancias})
df_importancias = df_importancias.sort_values(by='Importance', ascending=False)

# Exibir importâncias
print('Visualizando Importância das Variáveis')
print(80*'-')
print(df_importancias.head())
print(80*'-')

# Gráfico de barras da importância das variáveis
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=df_importancias)
plt.title('Feature Importance from Extra Trees')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()

# Salvando as Métricas
dict_modelo_ET = {
    'Nome do Modelo': 'Modelo 13 - V1 (Sem Ajuste de Hiperparâmetros)', 
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Extra Trees', 
    'ROC_AUC Score': roc_auc_ET,
    'AUC Score': auc_ET,
    'Acuracia': acuracia_ET
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_ET])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)











from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# Criando os modelos individuais
modelo_LR = LogisticRegression(random_state=42)
modelo_DT = DecisionTreeClassifier(random_state=42)
modelo_RF = RandomForestClassifier(random_state=42)
modelo_SVM = SVC(probability=True, random_state=42)
modelo_NB = GaussianNB()

# Criando o Voting Classifier
modelo_voting = VotingClassifier(estimators=[
    ('lr', modelo_LR),
    ('dt', modelo_DT),
    ('rf', modelo_RF),
    ('svm', modelo_SVM),
    ('nb', modelo_NB)
], voting='soft')

# Treinamento do modelo
modelo_voting.fit(X_treino_padronizado, y_treino)

# Visualizando o modelo treinado
print(modelo_voting)

## Previsões

# Previsões com dados de teste
y_pred_voting = modelo_voting.predict(X_teste_padronizado)
print('Previsões de Classe:')
print(y_pred_voting[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade para cada classe
y_pred_proba_voting = modelo_voting.predict_proba(X_teste_padronizado)
print('Previsões de Probabilidade para Cada Classe:')
print(y_pred_proba_voting[:5])
print('\n' + '-' * 80 + '\n')

# Obtemos as previsões no formato de probabilidade filtrando para a classe positiva (Cálculo da Curva ROC)
y_pred_proba_voting_pos = modelo_voting.predict_proba(X_teste_padronizado)[:, 1]
print('Previsões de Probabilidade para a Classe Positiva:')
print(y_pred_proba_voting_pos[:5])
print('\n' + '-' * 80 + '\n')

## Avaliação do Modelo

# Matriz de confusão
conf_matrix_voting = confusion_matrix(y_teste, y_pred_voting)
print('Matriz de Confusão:')
print(conf_matrix_voting)
print('\n' + '-' * 80 + '\n')

# Extraindo cada valor da Confusion Matrix
tn_voting, fp_voting, fn_voting, tp_voting = conf_matrix_voting.ravel()

# Calcula a métrica global AUC (Area Under The Curve) com dados reais e previsões em teste
roc_auc_voting = roc_auc_score(y_teste, y_pred_proba_voting_pos)

# Calcula a curva ROC com dados e previsões em teste
fpr_voting, tpr_voting, thresholds_voting = roc_curve(y_teste, y_pred_proba_voting_pos)

# AUC em teste
auc_voting = auc(fpr_voting, tpr_voting)

# Acurácia em teste
acuracia_voting = accuracy_score(y_teste, y_pred_voting)

# Exibindo as métricas
print(f'Acurácia: {acuracia_voting:.4f}')
print(f'ROC AUC Score: {roc_auc_voting:.4f}')
print(f'AUC Score: {auc_voting:.4f}')

# Salvando as Métricas
dict_modelo_voting = {
    'Nome do Modelo': 'Modelo 14 - V1 (Sem Ajuste de Hiperparâmetros)',
    'Tipo de Dado': 'Padronizado',
    'Nome do Algoritmo': 'Voting Classifier', 
    'ROC_AUC Score': roc_auc_voting,
    'AUC Score': auc_voting,
    'Acuracia': acuracia_voting
}

# Adicionando as métricas ao DataFrame existente
df_modelos = pd.concat([df_modelos, pd.DataFrame([dict_modelo_voting])], ignore_index=True)

# Verifica as métricas salvas
display(df_modelos)





# Ordenando o DataFrame pelo AUC Score
df_modelos_sorted = df_modelos.sort_values(by='AUC Score', ascending=False).reset_index(drop=True)

# Visualizando Daframe
display(df_modelos_sorted)















